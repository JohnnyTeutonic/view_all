{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gala import evaluate as ev, imio, viz, morpho, agglo, classify, features\n",
    "from skimage.util import regular_seeds\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/johnnyt/Documents/research_project_files/\")\n",
    "raw, gt = imio.read_cremi(\"Cremi_Data/sample_B_20160501.hdf\", datasets=['volumes/raw', 'volumes/labels/neuron_ids'])\n",
    "bpm = imio.read_h5_stack('raw_slice_1_Probabilities.h5', group='bpm_raw_b')\n",
    "membrane_prob = bpm[..., 2]\n",
    "train_slice = (slice(0, 15), slice(0, 480), slice(0, 480))\n",
    "test_slice = (slice(0, 15), slice(480, 960), slice(480, 960))\n",
    "gt_larger_2 = gt[train_slice]\n",
    "raw_larger_2 = 1-raw[train_slice]/255\n",
    "raw_test_slice = raw[test_slice]\n",
    "ws_larger_seeds_2 = regular_seeds(raw_larger_2[0].shape, n_points=700)\n",
    "ws_larger_seeds_2 = np.broadcast_to(ws_larger_seeds_2, raw_larger_2.shape)\n",
    "ws_larger_water = morpho.watershed_sequence(membrane_prob[train_slice], ws_larger_seeds_2, n_jobs=-1)\n",
    "raw_larger_testing_2 = membrane_prob[test_slice]\n",
    "gt_larger_testing_2 = gt[test_slice]\n",
    "gg = np.argsort(np.bincount(gt_larger_2.astype(int).ravel()))[-10:]\n",
    "sparse_large = imio.extract_segments(gt_larger_2, ids = gg)\n",
    "ws_larger_testing_2 = morpho.watershed_sequence(raw_larger_testing_2, ws_larger_seeds_2, n_jobs=-1)\n",
    "fm = features.moments.Manager()\n",
    "fh = features.histogram.Manager()\n",
    "fc = features.base.Composite(children=[fm, fh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_train_larger_2 = agglo.Rag(ws_larger_water, bpm[train_slice], feature_manager=fc)\n",
    "(X2, y2, w2, merges2) = g_train_larger_2.learn_agglomerate(gt_larger_2, fc, classifier='logistic')[0]\n",
    "y2 = y2[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_log_large_2 = classify.get_classifier('logistic').fit(X2,y2)\n",
    "learned_policy_large_2 = agglo.classifier_probability(fc, rf_log_large_2)\n",
    "g_test_large_2 = agglo.Rag(ws_larger_testing_2, bpm[test_slice], feature_manager=fc, merge_priority_function=learned_policy_large_2)\n",
    "g_test_large_2.agglomerate(np.inf)\n",
    "seg_stack_large_2 = [g_test_large_2.get_segmentation(t) for t in np.arange(0,1, 0.01)]\n",
    "split_vi_score_large_2 = [ev.split_vi(seg_stack_large_2[t], gt_larger_testing_2) for t in range(len(seg_stack_large_2))]\n",
    "split_vi_array_2 = np.array(split_vi_score_large_2)\n",
    "best_seg_ind = np.argmin(split_vi_array_2.sum(axis=1))\n",
    "best_seg = seg_stack_large_2[best_seg_ind]\n",
    "imio.write_h5_stack(npy_vol=split_vi_array_2, compression='lzf', fn='stack_of_segs_20_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2)\n",
    "ax0.plot(split_vi_array_2[:, 1], split_vi_array_2[:, 0])\n",
    "target_segs = np.argsort(np.bincount(seg_stack_large_2[9].astype(int).ravel()))[-10:]\n",
    "ax0.set_title(\"Split VI\")\n",
    "ax1.set_title(\"Cumulative VI\")\n",
    "ax1.plot(split_vi_array_2.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idxs, split_errs, merge_idxs, merge_errs = ev.sorted_vi_components(best_seg, gt_larger_testing_2)\n",
    "split_idxs_sorted, merge_idxs_sorted = np.argsort(split_idxs), np.argsort(merge_idxs)\n",
    "split_err_unsorted, merge_err_unsorted = split_errs[split_idxs_sorted], merge_errs[merge_idxs_sorted]\n",
    "split_err_img, merge_err_img = split_err_unsorted[gt_larger_testing_2], merge_err_unsorted[best_seg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2)\n",
    "ax[0,0].set_title(\"Worst splits\")\n",
    "ax[0,1].set_title(\"Wost merges\")\n",
    "ax[1,0].set_title(\"GT\")\n",
    "ax[1,1].set_title(\"Best seg\")\n",
    "viz.imshow_magma(split_err_img[12, ...], axis= ax[0,0])\n",
    "viz.imshow_magma(merge_err_img[12, ...], axis= ax[0,1])\n",
    "viz.imshow_rand(gt[test_slice][12, ...], axis=ax[1,0])\n",
    "viz.imshow_rand(best_seg[12, ...], axis=ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cont_table = ev.contingency_table(best_seg, gt[test_slice])\n",
    "worst_merge_comps = ev.split_components(merge_idxs[0], num_elems=10, cont=cont_table.T, axis=0)\n",
    "worst_merge_array = np.array(worst_merge_comps[0:3], dtype=np.int64)\n",
    "worst_merge_array[:, 0]\n",
    "np.cumsum(np.array(worst_merge_comps)[:, 1])\n",
    "extracted_worst_merge_comps = imio.extract_segments(ids=worst_merge_array[:, 0], seg=best_seg)\n",
    "imio.write_vtk(extracted_worst_merge_comps, fn='worst_merge_comps_20_10.vtk',spacing=[4, 4, 40])\n",
    "imio.write_vtk(raw_test_slice, fn='raw_test_slice_20_10.vtk',spacing=[4, 4, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "pool = mp.Pool(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5min 30s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 pool.apply_async(g_train_larger_2.learn_agglomerate(gt_larger_2, fc, classifier='logistic')[0],(X2, y2, w2, merges2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
