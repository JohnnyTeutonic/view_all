{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import tempfile\n",
    "import line_profiler as lp\n",
    "import multiprocessing as mp\n",
    "from gala import evaluate as ev, imio, viz, morpho, agglo, classify, features\n",
    "from skimage.segmentation import join_segmentations\n",
    "from skimage.util import regular_seeds\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10934/10934 [00:43<00:00, 249.80it/s] \n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# bpm segmentation\n",
    "os.chdir(\"/home/johnnyt/Documents/research_project_files/\")\n",
    "raw, gt = imio.read_cremi(\"Cremi_Data/sample_B_20160501.hdf\", datasets=['volumes/raw', 'volumes/labels/neuron_ids'])\n",
    "bpm = imio.read_h5_stack('raw_slice_1_Probabilities.h5', group='bpm_raw_b')\n",
    "membrane_prob = bpm[..., 2]\n",
    "train_slice = (slice(0, 15), slice(0, 480), slice(0, 480))\n",
    "test_slice = (slice(0, 15), slice(480, 960), slice(480, 960))\n",
    "gt_larger = gt[train_slice]\n",
    "raw_larger = 1-raw[train_slice]/255\n",
    "raw_test_slice = raw[test_slice]\n",
    "raw_bpm_testing = membrane_prob[test_slice]\n",
    "gt_raw_testing = gt[test_slice]\n",
    "ws_larger_seeds = regular_seeds(raw_larger[0].shape, n_points=700)\n",
    "ws_larger_seeds = np.broadcast_to(ws_larger_seeds, raw_larger.shape)\n",
    "ws_larger_train = morpho.watershed_sequence(membrane_prob[train_slice], ws_larger_seeds, n_jobs=-1)\n",
    "gg = np.argsort(np.bincount(gt_larger.astype(int).ravel()))[-10:]\n",
    "sparse_large = imio.extract_segments(gt_larger, ids = gg)\n",
    "ws_larger_testing = morpho.watershed_sequence(raw_bpm_testing, ws_larger_seeds, n_jobs=-1)\n",
    "fm = features.moments.Manager()\n",
    "fh = features.histogram.Manager()\n",
    "fc = features.base.Composite(children=[fm, fh])\n",
    "g_train_larger = agglo.Rag(ws_larger_train, bpm[train_slice], feature_manager=fc)\n",
    "(X, Y, W, Merges) = g_train_larger.learn_agglomerate(gt_larger, fc, classifier='logistic')[0]\n",
    "Y = Y[:, 0]\n",
    "rf_log_large = classify.get_classifier('logistic').fit(X,Y)\n",
    "learned_policy_large = agglo.classifier_probability(fc, rf_log_large)\n",
    "g_test_large_bpm = agglo.Rag(ws_larger_testing, bpm[test_slice], feature_manager=fc, \n",
    "                         merge_priority_function=learned_policy_large)\n",
    "g_test_large_bpm.agglomerate(np.inf)\n",
    "seg_stack_large_bpm = [g_test_large_bpm.get_segmentation(t) for t in np.arange(0,1, 0.1)]\n",
    "split_vi_score_bpm = [ev.split_vi(seg_stack_large_bpm[t], gt_raw_testing) for t in range(len(seg_stack_large_bpm))]\n",
    "split_vi_array_bpm = np.array(split_vi_score_bpm)\n",
    "best_seg_ind_bpm = np.argmin(split_vi_array_bpm.sum(axis=1))\n",
    "best_seg_bpm = seg_stack_large_bpm[best_seg_ind_bpm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bpm segmentation (largest)\n",
    "os.chdir(\"/home/johnnyt/Documents/research_project_files/\")\n",
    "raw, gt = imio.read_cremi(\"Cremi_Data/sample_B_20160501.hdf\", datasets=['volumes/raw', 'volumes/labels/neuron_ids'])\n",
    "bpm = imio.read_h5_stack('raw_slice_1_Probabilities.h5', group='bpm_raw_b')\n",
    "membrane_prob = bpm[..., 2]\n",
    "train_largest_slice = (slice(50, 80), slice(0, gt.shape[1]//2), slice(0, gt.shape[2]//2))\n",
    "test_largest_slice = (slice(80, 110), slice(gt.shape[1]//2, gt.shape[1]), slice(gt.shape[2]//2, gt.shape[2]))\n",
    "gt_largest = gt[train_largest_slice]\n",
    "raw_largest = 1-raw[train_largest_slice]/255\n",
    "raw_test_largest_slice = raw[test_largest_slice]\n",
    "raw_bpm_largest_testing = membrane_prob[test_largest_slice]\n",
    "gt_raw_largest_testing = gt[test_largest_slice]\n",
    "ratio = gt_largest.size/gt_larger.size\n",
    "multiplier = round(ratio*700)\n",
    "ws_largest_seeds = regular_seeds(raw_largest[0].shape, n_points=multiplier)\n",
    "ws_largest_seeds = np.broadcast_to(ws_largest_seeds, raw_largest.shape)\n",
    "ws_largest_train = morpho.watershed_sequence(membrane_prob[train_largest_slice], ws_largest_seeds, n_jobs=-1)\n",
    "ws_largest_testing = morpho.watershed_sequence(raw_bpm_largest_testing, ws_largest_seeds, n_jobs=-1)\n",
    "target_segs = np.argsort(np.bincount(gt_largest.astype(int).ravel()))[-10:]\n",
    "sparse_largest = imio.extract_segments(gt_largest, ids = target_segs)\n",
    "fm = features.moments.Manager()\n",
    "fh = features.histogram.Manager()\n",
    "fc = features.base.Composite(children=[fm, fh])\n",
    "g_train_largest = agglo.Rag(ws_largest_train, bpm[train_largest_slice], feature_manager=fc)\n",
    "(X, Y, W, Merges) = g_train_largest.learn_agglomerate(gt_largest, fc, classifier='logistic')[0]\n",
    "Y = Y[:, 0]\n",
    "rf_log_largest = classify.get_classifier('logistic').fit(X,Y)\n",
    "learned_policy_largest = agglo.classifier_probability(fc, rf_log_largest)\n",
    "test_largest_bpm = agglo.Rag(ws_largest_testing, bpm[test_largest_slice], feature_manager=fc, \n",
    "                         merge_priority_function=learned_policy_largest)\n",
    "test_largest_bpm.agglomerate(np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now()\n",
    "short_date = date.date()\n",
    "thresholds = np.arange(0, 1, 0.10)\n",
    "seg_stack_largest_bpm = (test_largest_bpm.get_segmentation(t) for t in thresholds)\n",
    "split_vi_score_largest_bpm = [ev.split_vi(seg, gt_raw_largest_testing) for seg in seg_stack_largest_bpm]\n",
    "split_vi_array_largest_bpm = np.array(split_vi_score_largest_bpm)\n",
    "best_threshold = thresholds[np.argmin(np.sum(split_vi_score_largest_bpm, axis=1))]\n",
    "best_seg_largest_bpm = test_largest_bpm.get_segmentation(best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "seg_stack_largest_bpm = [test_largest_bpm.get_segmentation(t) for t in np.arange(0,1, 0.05)]\n",
    "imio.write_h5_stack(npy_vol=seg_stack_largest_bpm, compression='lzf', fn=f'stack_of_segs_{short_date}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_segs_gt = np.argsort(np.bincount(gt_raw_largest_testing.astype(int).ravel()))[-10:]\n",
    "target_segs_auto = np.argsort(np.bincount(best_seg_largest_bpm.astype(int).ravel()))[-10:]\n",
    "sparse_largest_gt = imio.extract_segments(gt_raw_largest_testing, ids = target_segs_gt)\n",
    "sparse_largest_auto = imio.extract_segments(best_seg_largest_bpm, ids = target_segs_auto)\n",
    "imio.write_vtk(sparse_largest_auto, fn='sparse_largest_auto_05_11.vtk', spacing=[4,4,40])\n",
    "imio.write_vtk(sparse_largest_gt, fn='sparse_largest_gt_05_11.vtk', spacing=[4,4,40])\n",
    "imio.write_vtk(raw_test_largest_slice, fn=f'raw_test_slice_05_11.vtk',spacing=[4, 4, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10934/10934 [00:39<00:00, 275.43it/s] \n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# non-bpm segmentation\n",
    "\n",
    "ws_larger_testing = morpho.watershed_sequence(raw_larger, ws_larger_seeds, n_jobs=-1)\n",
    "g_train_larger = agglo.Rag(ws_larger_testing, raw_larger, feature_manager=fc)\n",
    "(X2, y2, w2, merges2) = g_train_larger.learn_agglomerate(gt_larger, fc, classifier='logistic')[0]\n",
    "y2 = y2[:, 0]\n",
    "rf_log_large = classify.get_classifier('logistic').fit(X2,y2)\n",
    "learned_policy_large = agglo.classifier_probability(fc, rf_log_large)\n",
    "g_test_large = agglo.Rag(ws_larger_testing, raw_larger, feature_manager=fc, merge_priority_function=learned_policy_large)\n",
    "g_test_large.agglomerate(np.inf)\n",
    "seg_stack_large = [g_test_large.get_segmentation(t) for t in np.arange(0,1, 0.10)]\n",
    "split_vi_score = [ev.split_vi(seg_stack_large[t], gt_raw_testing) for t in range(len(seg_stack_large))]\n",
    "split_vi_array = np.array(split_vi_score)\n",
    "best_seg_ind = np.argmin(split_vi_array.sum(axis=1))\n",
    "best_seg = seg_stack_large[best_seg_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import label2rgb\n",
    "joint_seg = join_segmentations(best_seg_bpm, gt_raw_testing)\n",
    "color = label2rgb(joint_seg, image_alpha=0.5)\n",
    "plt.imshow(color[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_idxs, split_errs, merge_idxs, merge_errs = ev.sorted_vi_components(best_seg_bpm, gt_raw_testing)\n",
    "split_idxs_sorted, merge_idxs_sorted = np.argsort(split_idxs), np.argsort(merge_idxs)\n",
    "split_err_unsorted, merge_err_unsorted = split_errs[split_idxs_sorted], merge_errs[merge_idxs_sorted]\n",
    "split_err_img, merge_err_img = split_err_unsorted[gt_raw_testing], merge_err_unsorted[best_seg_bpm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cont_table = ev.contingency_table(best_seg, gt_raw_testing)\n",
    "worst_merge_comps = ev.split_components(merge_idxs[0], num_elems=10, cont=cont_table.T, axis=0)\n",
    "worst_merge_array = np.array(worst_merge_comps[0:3], dtype=np.int64)\n",
    "worst_merge_array[:, 0]\n",
    "np.cumsum(np.array(worst_merge_comps)[:, 1])\n",
    "extracted_worst_merge_comps = imio.extract_segments(ids=worst_merge_array[:, 0], seg=best_seg)\n",
    "worst_split_comps = ev.split_components(split_idxs[0], num_elems=10, cont=cont_table.T, axis=0)\n",
    "worst_split_array = np.array(worst_split_comps[0:3], dtype=np.int64)\n",
    "worst_split_array[:, 0]\n",
    "np.cumsum(np.array(worst_split_comps)[:, 1])\n",
    "extracted_worst_split_comps = imio.extract_segments(ids=worst_split_array[:, 0], seg=best_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now()\n",
    "short_date = date.date()\n",
    "merge_idxs_m, merge_errs_m = ev.sorted_vi_components(joint_seg, best_seg_bpm)[0:2]\n",
    "cont_table_m = ev.contingency_table(best_seg_bpm, joint_seg)\n",
    "worst_merge_comps_m = ev.split_components(merge_idxs_m[0], num_elems=10, cont=cont_table_m.T, axis=1)\n",
    "worst_merge_array_m = np.array(worst_merge_comps_m[0:3], dtype=np.int64)\n",
    "np.cumsum(np.array(worst_merge_comps_m)[:, 1])\n",
    "extracted_worst_merge_comps_m = imio.extract_segments(ids=worst_merge_array_m[:, 0], seg=joint_seg)\n",
    "imio.write_vtk(extracted_worst_merge_comps_m, fn=f'worst_merge_comps_m_{short_date}.vtk',spacing=[4, 4, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_idxs_s, split_errs_s = ev.sorted_vi_components(joint_seg, gt_raw_testing)[0:2]\n",
    "cont_table_s = ev.contingency_table(joint_seg, gt_raw_testing)\n",
    "worst_split_comps_s = ev.split_components(split_idxs_s[0], num_elems=10, cont=cont_table_s.T, axis=0)\n",
    "worst_split_array_s = np.array(worst_split_comps_s[0:3], dtype=np.int64)\n",
    "np.cumsum(np.array(worst_split_comps_s)[:, 1])\n",
    "extracted_worst_split_comps_s = imio.extract_segments(ids=worst_split_array_s[:, 0], seg=joint_seg)\n",
    "imio.write_vtk(extracted_worst_split_comps_s, fn=f'worst_split_comps_s_{short_date}.vtk',spacing=[4, 4, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2)\n",
    "ax0.plot(split_vi_array_bpm[:, 1], split_vi_array_bpm[:, 0])\n",
    "target_segs = np.argsort(np.bincount(seg_stack_large_bpm[9].astype(int).ravel()))[-10:]\n",
    "ax0.set_title(\"Split VI\")\n",
    "ax1.set_title(\"Cumulative VI\")\n",
    "ax1.plot(split_vi_array_bpm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2)\n",
    "ax[0,0].set_title(\"Worst splits\")\n",
    "ax[0,1].set_title(\"Wost merges\")\n",
    "ax[1,0].set_title(\"GT\")\n",
    "ax[1,1].set_title(\"Best seg\")\n",
    "viz.imshow_magma(split_err_img[12, ...], axis= ax[0,0])\n",
    "viz.imshow_magma(merge_err_img[12, ...], axis= ax[0,1])\n",
    "viz.imshow_rand(gt_raw_testing[12, ...], axis=ax[1,0])\n",
    "viz.imshow_rand(best_seg[12, ...], axis=ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f learn_gala learn_gala()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include all the variables in the formal parameter list. \n",
    "\n",
    "def write_out_info(stack_of_segs):\n",
    "    \"\"\"Write out vtk files of worst merge and worst split comps, and stack of agglomerated segmentations.\"\"\"\n",
    "    import datetime\n",
    "    date = datetime.datetime.now()\n",
    "    short_date = date.date()\n",
    "    target_segs = np.argsort(np.bincount(gt_largest.astype(int).ravel()))[-10:]\n",
    "    sparse_largest = imio.extract_segments(gt_largest, ids = target_segs)\n",
    "    imio.write_vtk(extracted_worst_merge_comps, fn=f'worst_merge_comps_{short_date}.vtk',spacing=[4, 4, 40])\n",
    "    imio.write_vtk(raw_test_slice, fn=f'raw_test_slice_{short_date}.vtk',spacing=[4, 4, 40])\n",
    "    imio.write_h5_stack(npy_vol=stack_of_segs, compression='lzf', fn=f'stack_of_segs_{short_date}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
